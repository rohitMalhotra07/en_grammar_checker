{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75eb7b55-126c-48c0-96bd-3667efbe66be",
   "metadata": {},
   "source": [
    "# Classes for data preparation and creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf1d9a4-1260-4dd4-88e0-22a75a35d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ea487d-0d53-4e70-8382-4900016f7b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51824f3c-46e7-4460-aca4-45bdec015ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from en_grammar_checker.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9038fb5-ed8a-48b4-961f-7ec2f305b52b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17b75ba-cb11-42e4-8f03-f82177515b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BertClassificationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cnfg,\n",
    "        df: pd.DataFrame,\n",
    "        is_test: bool = False,\n",
    "        input_clm: str = \"sentence\",\n",
    "        label_clm: str = \"label\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        cnfg: instance of Config class\n",
    "        df: dataframe of data with label\n",
    "        is_test: True if it for inference dataframe\n",
    "        input_clm: column name for sentences\n",
    "        label_clm: column name for label (dtype should not be object)\n",
    "        \"\"\"\n",
    "        # get tokenizer from model name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(cnfg.base_model_name)\n",
    "        self.df = df\n",
    "        self.is_test = is_test\n",
    "        self.cnfg = cnfg\n",
    "        self.input_clm = input_clm\n",
    "        self.label_clm = label_clm\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        row_tensors = []\n",
    "\n",
    "        encoded_dict = self.tokenizer.encode_plus(\n",
    "            row[self.input_clm],  # Sentence to encode.\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=self.cnfg.context_length,  # Pad & truncate all sentences.\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,  # Construct attn. masks.\n",
    "            return_tensors=\"pt\",  # Return pytorch tensors.\n",
    "        )\n",
    "\n",
    "        if self.is_test:\n",
    "            return (\n",
    "                encoded_dict[\"input_ids\"].squeeze(),\n",
    "                encoded_dict[\"attention_mask\"].squeeze(),\n",
    "            )\n",
    "        else:\n",
    "            label = torch.as_tensor(row[self.label_clm], dtype=torch.float32)\n",
    "            return (\n",
    "                encoded_dict[\"input_ids\"].squeeze(),\n",
    "                encoded_dict[\"attention_mask\"].squeeze(),\n",
    "                label,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b1231-8343-488d-8d98-9d310fb19b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_train_data_loader(\n",
    "    cnfg,\n",
    "    df,\n",
    "    input_clm: str = \"sentence\",\n",
    "    label_clm: str = \"label\",\n",
    "):\n",
    "    dataset = BertClassificationDataset(\n",
    "        cnfg, df, is_test=False, input_clm=input_clm, label_clm=label_clm\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        sampler=RandomSampler(dataset),  # Select batches randomly\n",
    "        batch_size=cnfg.train_batch_size,\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eeb6d8-2649-4f6d-8678-9996de47b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_val_data_loader(\n",
    "    cnfg,\n",
    "    df,\n",
    "    input_clm: str = \"sentence\",\n",
    "    label_clm: str = \"label\",\n",
    "):\n",
    "    dataset = BertClassificationDataset(\n",
    "        cnfg, df, is_test=False, input_clm=input_clm, label_clm=label_clm\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        sampler=SequentialSampler(dataset),  # Select batches sequentialy\n",
    "        batch_size=cnfg.val_batch_size,\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82780eae-4cc4-42a9-8470-2c36b4fa8ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_test_data_loader(\n",
    "    cnfg,\n",
    "    df,\n",
    "    input_clm: str = \"sentence\",\n",
    "    label_clm=None,\n",
    "):\n",
    "    dataset = BertClassificationDataset(\n",
    "        cnfg, df, is_test=True, input_clm=input_clm, label_clm=label_clm\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        sampler=SequentialSampler(dataset),  # Select batches sequentialy\n",
    "        batch_size=cnfg.test_batch_size,\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9426fae-0bf6-4c90-9fed-3e2eccd0f64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6ea0d7a-2476-47de-aaee-542fe7b46471",
   "metadata": {},
   "source": [
    "### Testing DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905003ff-e819-4ab5-8af3-b795a36f0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbfea46-12fb-4202-b7f5-89e659e6ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\n",
    "    f\"{cnfg.train_path}\",\n",
    "    delimiter=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"sentence_source\", \"label\", \"label_notes\", \"sentence\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fdab50-b242-424c-9363-7d97f93ff8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee9f7d1-6e18-4121-b77e-6b97e6f09b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/Desktop/rohit/virtualenvs/rohit_transformers/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/rohit/Desktop/rohit/virtualenvs/rohit_transformers/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = get_train_data_loader(cnfg, df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef243a-e59e-4a3f-97ac-500e86765272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/rohit/Desktop/rohit/virtualenvs/rohit_transformers/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataloader_iterator = iter(train_dataloader)\n",
    "X, X2, Y = next(train_dataloader_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23221d3-ada3-4522-8b47-93da73cf7292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 512]), torch.Size([32, 512]), torch.Size([32]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X2.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45648c19-58a3-4e03-864a-ef09e472e91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  585, 1234,  ...,    0,    0,    0],\n",
       "        [   1,  273,  481,  ...,    0,    0,    0],\n",
       "        [   1,  512,  313,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   1,  273, 1659,  ...,    0,    0,    0],\n",
       "        [   1,  918, 3721,  ...,    0,    0,    0],\n",
       "        [   1, 1887,  261,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6013ed96-30a4-4072-93a4-f2fad7b47f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505abea5-c993-4f12-83ea-98bb14cda37c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
