[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "en_grammar_checker",
    "section": "",
    "text": "Base model: DeBERTaV3\nFinetuning DeBERTaV3 on CoLA dataset : https://www.kaggle.com/datasets/krazy47/cola-the-corpus-of-linguistic-acceptability/data + GED dataset merge\ncheckout docs: https://rohitmalhotra07.github.io/en_grammar_checker/",
    "crumbs": [
      "en_grammar_checker"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "en_grammar_checker",
    "section": "Install",
    "text": "Install\npip install en_grammar_checker",
    "crumbs": [
      "en_grammar_checker"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "en_grammar_checker",
    "section": "How to use",
    "text": "How to use\nCheck inference file in nbs folder",
    "crumbs": [
      "en_grammar_checker"
    ]
  },
  {
    "objectID": "index.html#results",
    "href": "index.html#results",
    "title": "en_grammar_checker",
    "section": "Results",
    "text": "Results\n\nCheck inference file in nbs folder\n      OR\nhttps://rohitmalhotra07.github.io/en_grammar_checker/inference.html#classification-reports",
    "crumbs": [
      "en_grammar_checker"
    ]
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "Custom DeBERTA V3",
    "section": "",
    "text": "Exported source\nimport math\n\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\n\n\n\nimport pandas as pd\n\nfrom en_grammar_checker.config import Config\nfrom en_grammar_checker.datasets import get_train_data_loader\n\n\n# checking model config\nmodel_config = AutoConfig.from_pretrained(cnfg.base_model_name)\n\n/home/rohit/Desktop/rohit/virtualenvs/rohit_transformers/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\n# checking tokenizer\nmy_tokenizer = AutoTokenizer.from_pretrained(cnfg.base_model_name)\n\n/home/rohit/Desktop/rohit/virtualenvs/rohit_transformers/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n\n\n\n# dir(my_tokenizer)\n\n\nmy_tokenizer.encode_plus\n\n&lt;bound method PreTrainedTokenizerBase.encode_plus of DebertaV2TokenizerFast(name_or_path='microsoft/deberta-v3-large', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n    128000: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}&gt;\n\n\n\na = my_tokenizer(\"I am done.\")\nb = my_tokenizer(\"Thanks! I am done\")\nc = my_tokenizer(\"I am done!\")\nd = my_tokenizer(\"[CLS]I am done!\")\n\nprint(a)\nprint(b)\nprint(c)\nprint(d)\n\n{'input_ids': [1, 273, 481, 619, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n{'input_ids': [1, 1195, 300, 273, 481, 619, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n{'input_ids': [1, 273, 481, 619, 300, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n{'input_ids': [1, 1, 273, 481, 619, 300, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n\n\n\n# Note: Tokenizer adds a starting token [CLS] and end of sentence token on its own\n\n\nCheck Model Architecture\n\nbase_model = AutoModel.from_pretrained(cnfg.base_model_name, config=model_config)\nbase_model.requires_grad_(False)  # Freeze the pretrained weights\n\nDebertaV2Model(\n  (embeddings): DebertaV2Embeddings(\n    (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n    (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n    (dropout): StableDropout()\n  )\n  (encoder): DebertaV2Encoder(\n    (layer): ModuleList(\n      (0-23): 24 x DebertaV2Layer(\n        (attention): DebertaV2Attention(\n          (self): DisentangledSelfAttention(\n            (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (pos_dropout): StableDropout()\n            (dropout): StableDropout()\n          )\n          (output): DebertaV2SelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n            (dropout): StableDropout()\n          )\n        )\n        (intermediate): DebertaV2Intermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): DebertaV2Output(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n          (dropout): StableDropout()\n        )\n      )\n    )\n    (rel_embeddings): Embedding(512, 1024)\n    (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n  )\n)\n\n\n\n# base_model\n\n\ndf_train = pd.read_csv(\n    f\"{cnfg.train_path}\",\n    delimiter=\"\\t\",\n    header=None,\n    names=[\"sentence_source\", \"label\", \"label_notes\", \"sentence\"],\n)\ntrain_dataloader = get_train_data_loader(cnfg, df_train)\ntrain_dataloader_iterator = iter(train_dataloader)\nX, X2, Y = next(train_dataloader_iterator)\n\n/home/rohit/Desktop/rohit/virtualenvs/rohit_transformers/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/home/rohit/Desktop/rohit/virtualenvs/rohit_transformers/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n\n\n\noutput_ = base_model(input_ids=X, attention_mask=X2)\n\n\n# dir(output_)\n\n\noutput_.last_hidden_state.shape\n\ntorch.Size([8, 512, 1024])\n\n\n\n# output_\n\n\n# Taking embeddings of [CLS] token\ncls_embedding = output_.last_hidden_state[:, 0:1, :].squeeze(1)\n\n\ncls_embedding.shape\n\ntorch.Size([8, 1024])\n\n\n\n# cls_embedding\n\n\nmodel_config.pooler_hidden_size\n\n1024\n\n\n\nsource\n\n\nEnDeBertaClassifier\n\n EnDeBertaClassifier (cnfg)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\nExported source\nclass EnDeBertaClassifier(nn.Module):\n    def __init__(self, cnfg):\n        super().__init__()\n\n        self.model_config = AutoConfig.from_pretrained(cnfg.base_model_name)\n        self.base_model = AutoModel.from_pretrained(\n            cnfg.base_model_name, config=self.model_config\n        )\n\n        self.base_model.requires_grad_(False)  # Freeze the pretrained weights\n        ## Classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(\n                self.model_config.pooler_hidden_size,\n                self.model_config.pooler_hidden_size,\n            ),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(\n                self.model_config.pooler_hidden_size,\n                128,\n            ),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            # nn.Linear(self.model_config.pooler_hidden_size, cnfg.num_classes),\n            nn.Linear(128, cnfg.num_classes),\n        )\n\n    def forward(self, input_ids, attention_mask):\n        base_embeddings = self.base_model(\n            input_ids=input_ids, attention_mask=attention_mask\n        )\n        cls_embedding = base_embeddings.last_hidden_state[:, 0:1, :].flatten(\n            start_dim=1\n        )  # Taking embeddings of [CLS] token\n\n        logits = self.classifier(cls_embedding)\n\n        return logits\n\n\n\nmy_model = EnDeBertaClassifier(cnfg)\n\n\nmy_model\n\nEnDeBertaClassifier(\n  (base_model): DebertaV2Model(\n    (embeddings): DebertaV2Embeddings(\n      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n      (dropout): StableDropout()\n    )\n    (encoder): DebertaV2Encoder(\n      (layer): ModuleList(\n        (0-23): 24 x DebertaV2Layer(\n          (attention): DebertaV2Attention(\n            (self): DisentangledSelfAttention(\n              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (pos_dropout): StableDropout()\n              (dropout): StableDropout()\n            )\n            (output): DebertaV2SelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n              (dropout): StableDropout()\n            )\n          )\n          (intermediate): DebertaV2Intermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaV2Output(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n            (dropout): StableDropout()\n          )\n        )\n      )\n      (rel_embeddings): Embedding(512, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n    )\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=1024, out_features=1024, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=1024, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.1, inplace=False)\n    (6): Linear(in_features=128, out_features=2, bias=True)\n  )\n)\n\n\n\nlogits = my_model(input_ids=X, attention_mask=X2)\n\n\nlogits.shape\n\ntorch.Size([8, 2])\n\n\n\nlogits\n\ntensor([[-0.0836,  0.0493],\n        [-0.0009,  0.0343],\n        [-0.0101,  0.0113],\n        [ 0.0093, -0.0483],\n        [-0.0530,  0.0029],\n        [ 0.0071,  0.0551],\n        [-0.0255,  0.0247],\n        [-0.0515,  0.0383]], grad_fn=&lt;AddmmBackward0&gt;)",
    "crumbs": [
      "Custom DeBERTA V3"
    ]
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Classes for data preparation and creating datasets",
    "section": "",
    "text": "Exported source\nimport os\n\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom transformers import AutoTokenizer\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\n\nfrom en_grammar_checker.config import Config\n\n\nsource\n\nBertClassificationDataset\n\n BertClassificationDataset (cnfg, df:pandas.core.frame.DataFrame,\n                            is_test:bool=False, input_clm:str='sentence',\n                            label_clm:str='label')\n\n*An abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader. Subclasses could also optionally implement :meth:__getitems__, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.*\n\n\nExported source\nclass BertClassificationDataset(Dataset):\n    def __init__(\n        self,\n        cnfg,\n        df: pd.DataFrame,\n        is_test: bool = False,\n        input_clm: str = \"sentence\",\n        label_clm: str = \"label\",\n    ):\n        \"\"\"\n        cnfg: instance of Config class\n        df: dataframe of data with label\n        is_test: True if it for inference dataframe\n        input_clm: column name for sentences\n        label_clm: column name for label (dtype should not be object)\n        \"\"\"\n        # get tokenizer from model name\n        self.tokenizer = AutoTokenizer.from_pretrained(cnfg.base_model_name)\n        self.df = df\n        self.is_test = is_test\n        self.cnfg = cnfg\n        self.input_clm = input_clm\n        self.label_clm = label_clm\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        row_tensors = []\n\n        encoded_dict = self.tokenizer.encode_plus(\n            row[self.input_clm],  # Sentence to encode.\n            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n            max_length=self.cnfg.context_length,  # Pad & truncate all sentences.\n            truncation=True,\n            padding=\"max_length\",\n            return_attention_mask=True,  # Construct attn. masks.\n            return_tensors=\"pt\",  # Return pytorch tensors.\n        )\n\n        if self.is_test:\n            return (\n                encoded_dict[\"input_ids\"].squeeze(),\n                encoded_dict[\"attention_mask\"].squeeze(),\n            )\n        else:\n            label = torch.as_tensor(row[self.label_clm], dtype=torch.int64)\n            return (\n                encoded_dict[\"input_ids\"].squeeze(),\n                encoded_dict[\"attention_mask\"].squeeze(),\n                label,\n            )\n\n\n\nsource\n\n\nget_train_data_loader\n\n get_train_data_loader (cnfg, df, input_clm:str='sentence',\n                        label_clm:str='label')\n\n\n\nExported source\ndef get_train_data_loader(\n    cnfg,\n    df,\n    input_clm: str = \"sentence\",\n    label_clm: str = \"label\",\n):\n    dataset = BertClassificationDataset(\n        cnfg, df, is_test=False, input_clm=input_clm, label_clm=label_clm\n    )\n    dataloader = DataLoader(\n        dataset,\n        sampler=RandomSampler(dataset),  # Select batches randomly\n        batch_size=cnfg.train_batch_size,\n        num_workers=cnfg.num_workers,\n    )\n\n    return dataloader\n\n\n\nsource\n\n\nget_val_data_loader\n\n get_val_data_loader (cnfg, df, input_clm:str='sentence',\n                      label_clm:str='label')\n\n\n\nExported source\ndef get_val_data_loader(\n    cnfg,\n    df,\n    input_clm: str = \"sentence\",\n    label_clm: str = \"label\",\n):\n    dataset = BertClassificationDataset(\n        cnfg, df, is_test=False, input_clm=input_clm, label_clm=label_clm\n    )\n    dataloader = DataLoader(\n        dataset,\n        sampler=SequentialSampler(dataset),  # Select batches sequentialy\n        batch_size=cnfg.val_batch_size,\n        num_workers=cnfg.num_workers,\n    )\n\n    return dataloader\n\n\n\nsource\n\n\nget_test_data_loader\n\n get_test_data_loader (cnfg, df, input_clm:str='sentence', label_clm=None)\n\n\n\nExported source\ndef get_test_data_loader(\n    cnfg,\n    df,\n    input_clm: str = \"sentence\",\n    label_clm=None,\n):\n    dataset = BertClassificationDataset(\n        cnfg, df, is_test=True, input_clm=input_clm, label_clm=label_clm\n    )\n    dataloader = DataLoader(\n        dataset,\n        sampler=SequentialSampler(dataset),  # Select batches sequentialy\n        batch_size=cnfg.test_batch_size,\n        num_workers=cnfg.num_workers,\n    )\n\n    return dataloader\n\n\n\n\nTesting DataSets\n\ncnfg = Config()\n\n\ndf_train = pd.read_csv(\n    f\"{cnfg.train_path}\",\n    delimiter=\"\\t\",\n    header=None,\n    names=[\"sentence_source\", \"label\", \"label_notes\", \"sentence\"],\n)\n\n\ntrain_dataloader = get_train_data_loader(cnfg, df_train)\n\n/home/rohit/Desktop/rohit/virtualenvs/rohit_transformers/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/home/rohit/Desktop/rohit/virtualenvs/rohit_transformers/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n\n\n\ntrain_dataloader_iterator = iter(train_dataloader)\nX, X2, Y = next(train_dataloader_iterator)\n\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/home/rohit/Desktop/rohit/virtualenvs/rohit_transformers/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n\n\n\nX.shape, X2.shape, Y.shape\n\n(torch.Size([8, 512]), torch.Size([8, 512]), torch.Size([8]))\n\n\n\nY.view(-1).shape\n\ntorch.Size([8])\n\n\n\nX\n\ntensor([[   1,  585, 1234,  ...,    0,    0,    0],\n        [   1,  273,  481,  ...,    0,    0,    0],\n        [   1,  512,  313,  ...,    0,    0,    0],\n        ...,\n        [   1,  273, 1659,  ...,    0,    0,    0],\n        [   1,  918, 3721,  ...,    0,    0,    0],\n        [   1, 1887,  261,  ...,    0,    0,    0]])\n\n\n\nX2\n\ntensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]])",
    "crumbs": [
      "Classes for data preparation and creating datasets"
    ]
  },
  {
    "objectID": "config.html",
    "href": "config.html",
    "title": "Define Config file",
    "section": "",
    "text": "source\n\nConfig\n\n Config ()\n\nClass to define config for the project\n\n\nExported source\nclass Config:\n    \"Class to define config for the project\"\n    seed = 42\n    # base_dataset_path = \"../data/cola_public/raw/\"\n    base_dataset_path = \"../data/merged_data/\"\n    base_model_name = \"microsoft/deberta-v3-large\"\n\n    trained_model_path = \"../trained_models/en_grammar_checker1.pt\"\n\n    # train_path = f\"{base_dataset_path}in_domain_train.tsv\"\n    train_path = f\"{base_dataset_path}train_data.csv\"\n\n    train_val_split = 0.20\n    num_workers = 4\n\n    # Model Params\n    context_length = 512  # Maximum sentence length\n    num_classes = 2\n\n    # Training Params\n    training_logs_path = \"../training_logs\"\n    experiment_name = \"test_run2\"\n\n    train_batch_size = 32\n    val_batch_size = 32\n    test_batch_size = 1\n    num_epochs = 50\n    early_stopping_rounds = 7\n    # learning_rate = 2e-5\n    # learning_rate = 0.0002051162178825565\n    learning_rate = 2e-4\n    eps = 1e-8\n    weight_decay = 0.01\n    betas = (0.9, 0.999)\n    warmup_prop = 0.1",
    "crumbs": [
      "Define Config file"
    ]
  },
  {
    "objectID": "trainer.html",
    "href": "trainer.html",
    "title": "PL Trainer for training model",
    "section": "",
    "text": "Exported source\nimport lightning as pl\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nfrom en_grammar_checker.models import EnDeBertaClassifier\n\n\n\nfrom en_grammar_checker.config import Config\nfrom en_grammar_checker.datasets import get_train_data_loader\n\n\nsource\n\nMyLightningClassifierModel\n\n MyLightningClassifierModel (cnfg)\n\nHooks to be used in LightningModule.\n\n\nExported source\n# define the LightningModule\nclass MyLightningClassifierModel(pl.LightningModule):\n    def __init__(self, cnfg):\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.model = EnDeBertaClassifier(cnfg)\n\n        self.learning_rate = cnfg.learning_rate\n        self.eps = cnfg.eps\n        self.weight_decay = cnfg.weight_decay\n        self.betas = cnfg.betas\n        self.warmup_prop = cnfg.warmup_prop\n\n    def training_step(self, batch, batch_idx):\n        # training_step defines the train loop.\n        # it is independent of forward\n        x, x2, y = batch\n        # y = y.view(-1)\n        pred = self.model(input_ids=x, attention_mask=x2)\n        loss = loss_fct(pred, y)\n\n        # Logging to TensorBoard (if installed) by default\n        self.log(\n            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n        )\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss, pred, y, _f1, _precision, _recall = self._shared_eval_step(\n            batch, batch_idx\n        )\n        # metrics = {\n        #     \"val_loss\": loss,\n        #     \"val_f1\": _f1,\n        #     \"val_precision\": _precision,\n        #     \"val_recall\": _recall,\n        # }\n        self.log(\n            \"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n        )\n\n        self.log(\n            \"val_f1\", _f1, on_step=False, on_epoch=True, prog_bar=True, logger=True\n        )\n\n        self.log(\n            \"val_precision\",\n            _precision,\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n            logger=True,\n        )\n\n        self.log(\n            \"val_recall\",\n            _recall,\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n            logger=True,\n        )\n\n        self.validation_step_y_hat.append(pred)\n        self.validation_step_y.append(y)\n\n        return loss\n\n    def on_validation_epoch_start(self):\n        self.validation_step_y_hat = []  # free memory\n        self.validation_step_y = []\n\n    def on_validation_epoch_end(self):\n        all_preds = torch.cat(self.validation_step_y_hat)\n        all_y = torch.cat(self.validation_step_y)\n\n        all_preds = torch.argmax(all_preds, axis=1).flatten().detach().cpu()\n        all_y = all_y.flatten().detach().cpu()\n\n        # do something with all preds\n        _f1 = f1_score(all_preds, all_y, average=\"macro\")\n        _precision = precision_score(all_preds, all_y, average=\"macro\")\n        _recall = recall_score(all_preds, all_y, average=\"macro\")\n        # metrics = {\n        #     \"val_f1_full\": _f1,\n        #     \"val_precision_full\": _precision,\n        #     \"val_recall_full\": _recall,\n        # }\n        # self.log_dict(metrics)\n\n        print(\n            f\"\\t Epoch: {self.current_epoch}, Val F1: {_f1}, Val Precision: {_precision}, Val Recall {_recall}\"\n        )\n\n    def _shared_eval_step(self, batch, batch_idx):\n        x, x2, y = batch\n        # y = y.reshape(-1, 1)\n        pred = self.model(input_ids=x, attention_mask=x2)\n        loss = loss_fct(pred, y)\n\n        _f1 = f1_score(\n            torch.argmax(pred, axis=1).cpu().numpy(), y.cpu().numpy(), average=\"macro\"\n        )\n        _precision = precision_score(\n            torch.argmax(pred, axis=1).cpu().numpy(), y.cpu().numpy(), average=\"macro\"\n        )\n\n        _recall = recall_score(\n            torch.argmax(pred, axis=1).cpu().numpy(), y.cpu().numpy(), average=\"macro\"\n        )\n        return loss, pred, y, _f1, _precision, _recall\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n        x, x2 = batch\n        pred = self.model(x)\n        return pred\n\n    def configure_optimizers(self):\n        # return optimizer\n        optimizer = AdamW(\n            self.model.parameters(),\n            lr=self.learning_rate,\n            eps=self.eps,\n            weight_decay=self.weight_decay,\n            betas=self.betas,\n        )\n        # get_linear_schedule_with_warmup(\n        #     optimizer,\n        #     num_warmup_steps=len(self.train_dataloader)\n        #     * self.num_epochs\n        #     * self.warmup_prop,\n        #     num_training_steps=len(self.train_dataloader) * self.num_epochs,\n        # )\n\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer,\n            patience=5,\n            factor=0.1,\n        )\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": scheduler,\n            \"monitor\": \"train_loss\",\n        }",
    "crumbs": [
      "PL Trainer for training model"
    ]
  },
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "Inference",
    "section": "",
    "text": "import pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\n\nfrom en_grammar_checker.config import Config\nfrom en_grammar_checker.trainer import MyLightningClassifierModel\ntqdm.pandas()\ncnfg = Config()\n# model = torch.load(cnfg.trained_model_path)\n# model.keys()\npl_module = MyLightningClassifierModel.load_from_checkpoint(cnfg.trained_model_path)\n\n/home/rohit/Desktop/rohit/virtualenvs/rohit_transformers/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n# get tokenizer\n# my_tokenizer = AutoTokenizer.from_pretrained(cnfg.base_model_name)\n# or\nmy_tokenizer = AutoTokenizer.from_pretrained(\n    pl_module.model.base_model.config._name_or_path\n)\n\n/home/rohit/Desktop/rohit/virtualenvs/rohit_transformers/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/home/rohit/Desktop/rohit/virtualenvs/rohit_transformers/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\ncnfg.base_model_name\n\n'microsoft/deberta-v3-large'\npl_module.model.base_model.config._name_or_path\n\n'microsoft/deberta-v3-large'\ndevice = pl_module.device\npl_module = pl_module.eval()\ndef get_encoded_tensor(context_length, my_tokenizer, sentence, device):\n    tokens_dict = my_tokenizer.encode_plus(\n        sentence,  # Sentence to encode.\n        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n        max_length=context_length,  # Pad & truncate all sentences.\n        truncation=True,\n        padding=\"max_length\",\n        return_attention_mask=True,  # Construct attn. masks.\n        return_tensors=\"pt\",\n    ).to(device)\n    input_ids = tokens_dict[\"input_ids\"]\n    attention_mask = tokens_dict[\"attention_mask\"]\n    return input_ids, attention_mask, tokens_dict\ndef infer_for_sentence(sentence, module, context_length, tokenizer, device):\n    input_ids, attention_mask, tokens_dict = get_encoded_tensor(\n        context_length, tokenizer, sentence, device\n    )\n    with torch.no_grad():\n        output = module.model(input_ids=input_ids, attention_mask=attention_mask)\n\n    probs = nn.Softmax(dim=1)(output).detach().cpu()\n    label = probs.argmax().item()\n    return output, probs, label\nsentence = \"She knew French for Tom.\"\noutput, probs, label = infer_for_sentence(\n    sentence, pl_module, cnfg.context_length, my_tokenizer, device\n)\noutput\n\ntensor([[-1.3638,  0.9380]], device='cuda:0')\nprobs\n\ntensor([[0.0910, 0.9090]])\nlabel\n\n1",
    "crumbs": [
      "Inference"
    ]
  },
  {
    "objectID": "inference.html#test-set-predictions",
    "href": "inference.html#test-set-predictions",
    "title": "Inference",
    "section": "Test set predictions",
    "text": "Test set predictions\n\ntest_set_1 = pd.read_csv(\n    f\"{cnfg.base_dataset_path}in_domain_dev.tsv\",\n    delimiter=\"\\t\",\n    header=None,\n    names=[\"sentence_source\", \"label\", \"label_notes\", \"sentence\"],\n)\n\ntest_set_2 = pd.read_csv(\n    f\"{cnfg.base_dataset_path}out_of_domain_dev.tsv\",\n    delimiter=\"\\t\",\n    header=None,\n    names=[\"sentence_source\", \"label\", \"label_notes\", \"sentence\"],\n)\n\n\ntest_set_1.head()\n\n\n\n\n\n\n\n\n\nsentence_source\nlabel\nlabel_notes\nsentence\n\n\n\n\n0\ngj04\n1\nNaN\nThe sailors rode the breeze clear of the rocks.\n\n\n1\ngj04\n1\nNaN\nThe weights made the rope stretch over the pul...\n\n\n2\ngj04\n1\nNaN\nThe mechanical doll wriggled itself loose.\n\n\n3\ncj99\n1\nNaN\nIf you had eaten more, you would want less.\n\n\n4\ncj99\n0\n*\nAs you eat the most, you want the least.\n\n\n\n\n\n\n\n\n\ntest_set_2.head()\n\n\n\n\n\n\n\n\n\nsentence_source\nlabel\nlabel_notes\nsentence\n\n\n\n\n0\nclc95\n1\nNaN\nSomebody just left - guess who.\n\n\n1\nclc95\n1\nNaN\nThey claimed they had settled on something, bu...\n\n\n2\nclc95\n1\nNaN\nIf Sam was going, Sally would know where.\n\n\n3\nclc95\n1\nNaN\nThey're going to serve the guests something, b...\n\n\n4\nclc95\n1\nNaN\nShe's reading. I can't imagine what.\n\n\n\n\n\n\n\n\n\ntest_set_1.shape, test_set_2.shape\n\n((527, 4), (516, 4))\n\n\n\ntest_set_1.label.value_counts()\n\nlabel\n1    365\n0    162\nName: count, dtype: int64\n\n\n\ntest_set_2.label.value_counts()\n\nlabel\n1    354\n0    162\nName: count, dtype: int64\n\n\n\ntest_set_1[\"pred\"] = test_set_1.sentence.progress_apply(\n    lambda x: infer_for_sentence(\n        x, pl_module, cnfg.context_length, my_tokenizer, device\n    )[2]\n)\n\n100%|█████████████████████████████████████████| 527/527 [00:25&lt;00:00, 20.52it/s]\n\n\n\ntest_set_2[\"pred\"] = test_set_2.sentence.progress_apply(\n    lambda x: infer_for_sentence(\n        x, pl_module, cnfg.context_length, my_tokenizer, device\n    )[2]\n)\n\n100%|█████████████████████████████████████████| 516/516 [00:25&lt;00:00, 20.26it/s]\n\n\n\ntest_set_1.pred.value_counts()\n\npred\n1    379\n0    148\nName: count, dtype: int64\n\n\n\ntest_set_2.pred.value_counts()\n\npred\n1    370\n0    146\nName: count, dtype: int64\n\n\n\ntest_set_2[test_set_2.label == 0]\n\n\n\n\n\n\n\n\n\nsentence_source\nlabel\nlabel_notes\nsentence\npred\n\n\n\n\n6\nclc95\n0\n*\nJohn ate dinner but I don't know who.\n1\n\n\n7\nclc95\n0\n*\nShe mailed John a letter, but I don't know to ...\n1\n\n\n10\nclc95\n0\n*\nShe was bathing, but I couldn't make out who.\n0\n\n\n11\nclc95\n0\n*\nShe knew French for Tom.\n1\n\n\n12\nclc95\n0\n*\nJohn is tall on several occasions.\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n493\nw_80\n0\n*\nIt is to give up to leave.\n0\n\n\n495\nw_80\n0\n*\nIt was believed to be illegal by them to do that.\n1\n\n\n501\nw_80\n0\n*\nI gave Pete the book to impress.\n1\n\n\n504\nw_80\n0\n*\nI presented Bill with it to read.\n1\n\n\n513\nw_80\n0\n*\nJohn bought a dog for himself to play with.\n1\n\n\n\n\n162 rows × 5 columns\n\n\n\n\n\n# test_set_3 = pd.read_excel(\"../data/ged_data/test_data.xlsx\")\ntest_set_3 = pd.read_csv(\"../data/ged_data/val_data.csv\")\n\n\ntest_set_3.labels.value_counts()\n\nlabels\n0    5000\n1    5000\nName: count, dtype: int64\n\n\n\ntest_set_3.shape\n\n(10000, 2)\n\n\n\ntest_set_3[\"pred\"] = test_set_3.input.progress_apply(\n    lambda x: infer_for_sentence(\n        x, pl_module, cnfg.context_length, my_tokenizer, device\n    )[2]\n)\n\n100%|█████████████████████████████████████| 10000/10000 [08:12&lt;00:00, 20.29it/s]\n\n\n\nClassification Reports\n\nclass_name = [\"Incorrect\", \"Correct\"]\n\n\nprint(\n    classification_report(\n        test_set_1.label.values, test_set_1.pred.values, target_names=class_name\n    )\n)\n\n              precision    recall  f1-score   support\n\n   Incorrect       0.76      0.70      0.73       162\n     Correct       0.87      0.90      0.89       365\n\n    accuracy                           0.84       527\n   macro avg       0.82      0.80      0.81       527\nweighted avg       0.84      0.84      0.84       527\n\n\n\n\nprint(\n    classification_report(\n        test_set_2.label.values, test_set_2.pred.values, target_names=class_name\n    )\n)\n\n              precision    recall  f1-score   support\n\n   Incorrect       0.77      0.70      0.73       162\n     Correct       0.87      0.91      0.89       354\n\n    accuracy                           0.84       516\n   macro avg       0.82      0.80      0.81       516\nweighted avg       0.84      0.84      0.84       516\n\n\n\n\nprint(\n    classification_report(\n        test_set_3.labels.values, test_set_3.pred.values, target_names=class_name\n    )\n)\n\n              precision    recall  f1-score   support\n\n   Incorrect       0.62      0.45      0.52      5000\n     Correct       0.57      0.73      0.64      5000\n\n    accuracy                           0.59     10000\n   macro avg       0.60      0.59      0.58     10000\nweighted avg       0.60      0.59      0.58     10000",
    "crumbs": [
      "Inference"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "training_pipeline.html",
    "href": "training_pipeline.html",
    "title": "Training Pipeline",
    "section": "",
    "text": "import lightning as pl\nimport pandas as pd\nfrom lightning.pytorch import loggers as pl_loggers\nfrom lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\n\nfrom en_grammar_checker.config import Config\nfrom en_grammar_checker.datasets import get_train_data_loader, get_val_data_loader\nfrom en_grammar_checker.trainer import MyLightningClassifierModel\n\n\ncnfg = Config()\n\n\nRead Data\n\n# df = pd.read_csv(\n#     f\"{cnfg.train_path}\",\n#     delimiter=\"\\t\",\n#     header=None,\n#     names=[\"sentence_source\", \"label\", \"label_notes\", \"sentence\"],\n# )\n\ndf = pd.read_csv(f\"{cnfg.train_path}\")\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nsentence\nlabel\n\n\n\n\n0\nAt the time , Japan suffered from chronic ener...\n1\n\n\n1\nBut I am not sad .\n1\n\n\n2\nWhat is the different between relevant to and ...\n0\n\n\n3\nMedea wondered if that the potion was ready\n0\n\n\n4\nThen I went a pharmacy to buy medicine for a c...\n0\n\n\n\n\n\n\n\n\n\nCreate Train Val DF\n\ntrain_df, val_df = train_test_split(\n    df, test_size=cnfg.train_val_split, random_state=cnfg.seed\n)\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\n\n\ntrain_df.shape, val_df.shape\n\n((28134, 2), (7034, 2))\n\n\n\n\nGet DataLoaders\n\ntrain_dataloader = get_train_data_loader(cnfg, train_df)\nval_dataloader = get_val_data_loader(cnfg, val_df)\n\n/home/rohit/Desktop/rohit/virtualenvs/rohit_transformers/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/home/rohit/Desktop/rohit/virtualenvs/rohit_transformers/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n\n\n\n\nPL Model\n\ntask = MyLightningClassifierModel(cnfg)\n\n\n\nTraining\n\ncheckpoint_callback = ModelCheckpoint(\n    # dirpath=f\"{cnfg.training_logs_path}/{cnfg.experiment_name}/\",\n    monitor=\"val_f1\",\n    filename=\"{epoch}-{val_f1:.4f}\",\n    every_n_train_steps=1,\n    mode=\"max\",\n    save_top_k=1,\n)\n\n\ntrainer = pl.Trainer(\n    accelerator=\"gpu\",\n    default_root_dir=f\"{cnfg.training_logs_path}/{cnfg.experiment_name}/\",\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_f1\",\n            mode=\"max\",\n            patience=cnfg.early_stopping_rounds,\n        ),\n        # RichProgressBar(),\n        checkpoint_callback,\n        # progress_bar,\n    ],\n    max_epochs=100,\n    # enable_progress_bar=False,\n)\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/home/rohit/Desktop/rohit/virtualenvs/rohit_transformers/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n\n\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ntrainer.fit(task, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n\nYou are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\nMissing logger folder: ../training_logs/test_run2/lightning_logs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name  | Type                | Params\n----------------------------------------------\n0 | model | EnDeBertaClassifier | 435 M \n----------------------------------------------\n1.2 M     Trainable params\n434 M     Non-trainable params\n435 M     Total params\n1,740.773 Total estimated model params size (MB)\n\n\n\n\n\n     Epoch: 0, Val F1: 0.3333333333333333, Val Precision: 0.5, Val Recall 0.25\n\n\n\n\n\n\n# from lightning.pytorch.tuner.tuning import Tuner\n# lr_finder = Tuner(trainer).lr_find(model = task,train_dataloaders=train_dataloader, val_dataloaders=val_dataloader,min_lr=1e-08, max_lr=0.1, num_training=1000)",
    "crumbs": [
      "Training Pipeline"
    ]
  },
  {
    "objectID": "EDA_dataset.html",
    "href": "EDA_dataset.html",
    "title": "rm_en_grammar_check",
    "section": "",
    "text": "df_train.head()\n\n\n\n\n\n\n\n\n\nsentence_source\nlabel\nlabel_notes\nsentence\n\n\n\n\n0\ngj04\n1\nNaN\nOur friends won't buy this analysis, let alone...\n\n\n1\ngj04\n1\nNaN\nOne more pseudo generalization and I'm giving up.\n\n\n2\ngj04\n1\nNaN\nOne more pseudo generalization or I'm giving up.\n\n\n3\ngj04\n1\nNaN\nThe more we study verbs, the crazier they get.\n\n\n4\ngj04\n1\nNaN\nDay by day the facts are getting murkier.\ndf_train.iloc[0].sentence\n\n\"Our friends won't buy this analysis, let alone the next one we propose.\"\ndf_train.sentence_source.value_counts()\n\nsentence_source\nks08      1745\nl-93      1294\nr-67       916\nad03       852\nc_13       778\nbc01       772\nsks13      573\nm_02       378\nb_73       238\ncj99       210\nd_98       162\nsgww85     139\nrhl07      134\ngj04        99\ng_81        99\nb_82        83\nkl93        79\nName: count, dtype: int64\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8551 entries, 0 to 8550\nData columns (total 4 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   sentence_source  8551 non-null   object\n 1   label            8551 non-null   int64 \n 2   label_notes      2527 non-null   object\n 3   sentence         8551 non-null   object\ndtypes: int64(1), object(3)\nmemory usage: 267.3+ KB\ndf_train.label.value_counts()\n\nlabel\n1    6023\n0    2528\nName: count, dtype: int64\ndf_train.sentence.apply(lambda x: len(x.split(\".\"))).value_counts()\n\nsentence\n2    7538\n1     990\n3      20\n4       3\nName: count, dtype: int64\n# df_train.sentence.apply(lambda x: len(x.split('.')))\ndf_train.sentence.apply(lambda x: len(x.split(\" \"))).describe()\n\ncount    8551.000000\nmean        7.696059\nstd         3.622946\nmin         2.000000\n25%         5.000000\n50%         7.000000\n75%         9.000000\nmax        42.000000\nName: sentence, dtype: float64",
    "crumbs": [
      "Prepare new Data"
    ]
  },
  {
    "objectID": "EDA_dataset.html#prepare-new-data",
    "href": "EDA_dataset.html#prepare-new-data",
    "title": "rm_en_grammar_check",
    "section": "Prepare new Data",
    "text": "Prepare new Data\n\ndata_2_df.head(1)\n\n\n\n\n\n\n\n\n\nsentence\nlabel\n\n\n\n\n0\nIt was really delicious ! !\n1",
    "crumbs": [
      "Prepare new Data"
    ]
  }
]