# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/datasets.ipynb.

# %% auto 0
__all__ = ['BertClassificationDataset', 'get_train_data_loader', 'get_val_data_loader', 'get_test_data_loader']

# %% ../nbs/datasets.ipynb 2
import pandas as pd
import torch
from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler
from transformers import AutoTokenizer

# %% ../nbs/datasets.ipynb 5
class BertClassificationDataset(Dataset):
    def __init__(
        self,
        cnfg,
        df: pd.DataFrame,
        is_test: bool = False,
        input_clm: str = "sentence",
        label_clm: str = "label",
    ):
        """
        cnfg: instance of Config class
        df: dataframe of data with label
        is_test: True if it for inference dataframe
        input_clm: column name for sentences
        label_clm: column name for label (dtype should not be object)
        """
        # get tokenizer from model name
        self.tokenizer = AutoTokenizer.from_pretrained(cnfg.base_model_name)
        self.df = df
        self.is_test = is_test
        self.cnfg = cnfg
        self.input_clm = input_clm
        self.label_clm = label_clm

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        row_tensors = []

        encoded_dict = self.tokenizer.encode_plus(
            row[self.input_clm],  # Sentence to encode.
            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'
            max_length=self.cnfg.context_length,  # Pad & truncate all sentences.
            pad_to_max_length=True,
            return_attention_mask=True,  # Construct attn. masks.
            return_tensors="pt",  # Return pytorch tensors.
        )

        if self.is_test:
            return (
                encoded_dict["input_ids"].squeeze(),
                encoded_dict["attention_mask"].squeeze(),
            )
        else:
            label = torch.as_tensor(row[self.label_clm], dtype=torch.float32)
            return (
                encoded_dict["input_ids"].squeeze(),
                encoded_dict["attention_mask"].squeeze(),
                label,
            )

# %% ../nbs/datasets.ipynb 6
def get_train_data_loader(
    cnfg,
    df,
    input_clm: str = "sentence",
    label_clm: str = "label",
):
    dataset = BertClassificationDataset(
        cnfg, df, is_test=False, input_clm=input_clm, label_clm=label_clm
    )
    dataloader = DataLoader(
        dataset,
        sampler=RandomSampler(dataset),  # Select batches randomly
        batch_size=cnfg.train_batch_size,
    )

    return dataloader

# %% ../nbs/datasets.ipynb 7
def get_val_data_loader(
    cnfg,
    df,
    input_clm: str = "sentence",
    label_clm: str = "label",
):
    dataset = BertClassificationDataset(
        cnfg, df, is_test=False, input_clm=input_clm, label_clm=label_clm
    )
    dataloader = DataLoader(
        dataset,
        sampler=SequentialSampler(dataset),  # Select batches sequentialy
        batch_size=cnfg.val_batch_size,
    )

    return dataloader

# %% ../nbs/datasets.ipynb 8
def get_test_data_loader(
    cnfg,
    df,
    input_clm: str = "sentence",
    label_clm=None,
):
    dataset = BertClassificationDataset(
        cnfg, df, is_test=True, input_clm=input_clm, label_clm=label_clm
    )
    dataloader = DataLoader(
        dataset,
        sampler=SequentialSampler(dataset),  # Select batches sequentialy
        batch_size=cnfg.test_batch_size,
    )

    return dataloader
