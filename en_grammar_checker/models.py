# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/models.ipynb.

# %% auto 0
__all__ = ['EnDeBertaClassifier']

# %% ../nbs/models.ipynb 2
import math

import torch
import torch.nn as nn
import transformers
from transformers import AutoConfig, AutoModel, AutoTokenizer

# %% ../nbs/models.ipynb 28
class EnDeBertaClassifier(nn.Module):
    def __init__(self, cnfg):
        super().__init__()

        self.model_config = AutoConfig.from_pretrained(cnfg.base_model_name)
        self.base_model = AutoModel.from_pretrained(
            cnfg.base_model_name, config=self.model_config
        )

        self.base_model.requires_grad_(False)  # Freeze the pretrained weights
        ## Classifier
        self.classifier = nn.Sequential(
            nn.Linear(
                self.model_config.pooler_hidden_size,
                self.model_config.pooler_hidden_size,
            ),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(
                self.model_config.pooler_hidden_size,
                128,
            ),
            nn.ReLU(),
            nn.Dropout(0.1),
            # nn.Linear(self.model_config.pooler_hidden_size, cnfg.num_classes),
            nn.Linear(128, cnfg.num_classes),
        )

    def forward(self, input_ids, attention_mask):
        base_embeddings = self.base_model(
            input_ids=input_ids, attention_mask=attention_mask
        )
        cls_embedding = base_embeddings.last_hidden_state[:, 0:1, :].flatten(
            start_dim=1
        )  # Taking embeddings of [CLS] token

        logits = self.classifier(cls_embedding)

        return logits
